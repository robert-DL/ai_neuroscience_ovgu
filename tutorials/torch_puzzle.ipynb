{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_puzzle",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on PyTorch\n",
        "\n",
        "As all of you wanted a little bit more hands-on, I created a little puzzle game. For each cell, I provide you the commands, and you have/ can to use them. When you click on the words, you will be directed to the torch page, where you can find information on the modules and commands. Some keys have no link, because they are connected to objects of the torch library. You can look them up then on the corresponding pages. Have fun :)"
      ],
      "metadata": {
        "id": "PdwJn16LCcqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install braindecode"
      ],
      "metadata": {
        "id": "n_XyJtM6tog9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5s4XL9dbCU_W"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from braindecode.visualization import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Normally, we would write our own little data class - here we use the predefined MNIST for brevity. MNIST consists of a collection of handwritten digits from 0-9. This is the most classic example used in Deep Learning and is easily to access, since you can visualize the data and labels easily together.\n",
        "\n",
        "Since I do not create a own dataset here, I paste a [link to a tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) on the PyTorch page where you can see how you work with the dataset class. You can also look into the \"Deep Learning on EEG Data\" colab notebook, to see how I did it (will be likely equal)."
      ],
      "metadata": {
        "id": "OucpSVOhDOad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get training data and test data\n",
        "training_data = FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor(),)\n",
        "test_data = FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor(),)\n",
        "\n",
        "# Plot the data (you can index into a dataset like a normal numpy array, tf.tensor ...)\n",
        "\n"
      ],
      "metadata": {
        "id": "x4hwTBTSDovd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation\n",
        "\n",
        "As you will do the classic image classification example yourself with torch, you can build a convolutional network as crazy as you want - it will score great. However, you can also try a fully connected network. This boils down to your experience and how much time you have. \n",
        "\n",
        "### Key Commands\n",
        "\n",
        "[`torch.nn.Module()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), [`torch.nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d), [`torch.nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear), [`torch.nn.Softmax()`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax), [`torch.nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html), `def forward(self, x): ...`, [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "\n",
        "Hint: Output size after convolution:\n",
        "\\begin{equation}\n",
        "out = ⌊ \\frac{input\\_dim - kernel\\_size}{stride} ⌋ + 1\n",
        "\\end{equation}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H5chIXaAO4rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class MyModel():\n",
        "  def __init__(self, *unusedargs):\n",
        "    super().__init__()\n",
        "\n",
        "    # My models layers come here\n",
        "    # --------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the model \n",
        "my_model = MyModel()\n",
        "\n",
        "# Check if model is working\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o1dmofDBO3N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Environment and Train\n",
        "\n",
        "We will chose a loss function here, set up the optimizer and take care of the data loading process. Also, we will wrap everything into a training and a validation method (you can extend this for your project to this wrapper class style you saw in the tutorial). If you do this once, you are avoiding a lot of boilerplate code later on. \n",
        "\n",
        "### Key Commands \n",
        "[`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html), [`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD), [`torch.optim.scheduler`](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate), `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`, [`torch.utils.data.DataLoader()`](https://pytorch.org/docs/stable/data.html#multi-process-data-loading), `loss.item()`, `train_step(...)`, `val_step(...)`, [`torch.utils.data.random_split()`](https://pytorch.org/docs/stable/data.html#multi-process-data-loading)\n",
        "\n",
        "If you want to use the gpu: `.to(\"cuda\")`"
      ],
      "metadata": {
        "id": "eqi9TnBnlRwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(loader, model, optimizer, loss_fn):\n",
        "  pass\n",
        "\n",
        "def val_step(loader, model, loss_fn):\n",
        "  pass\n",
        "\n",
        "# Setting the framework\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "# Wrap the dataset for easy loading\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "# Train\n",
        "n_epochs = 2\n",
        "train_loss, val_loss = [], []\n",
        "for epoch in range(n_epochs):\n",
        "\n"
      ],
      "metadata": {
        "id": "vucjuRMbmk6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on test set\n",
        "\n",
        "We can evaluate on the test set, by simply predicting the labels and creating a confusion matrix. You can create a confusion matrix as in the tutorial on Deep Learning on EEG Data :). I already added pip install braindecode and imported sklearn.metrics.confusion_matrix and the plotting routine from braindecode.\n",
        "\n",
        "You may want to use `with torch.no_grad():` around the for loop ;) "
      ],
      "metadata": {
        "id": "1tXP7eA-rgCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "trLJUzJNmvJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}