{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning EEG-Data",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for EEG - Data\n",
        "\n",
        "This tutorial is two-fold. One the one hand, we will go through a simple example on how to train a model with the braindecode environment. This is particularly useful for you, as cou can have benchmarks to compare with. On the other hand, we will look into setting up our own dataset with torch and create a small model. The latter part is not new to everyone, especially if you looked into the tutorial we provided. However, in order to get a feeling for how to work with EEG-Data, this should be useful. \n",
        "\n",
        "For the braindecode walkthrough, we simply follow the steps shown [here](https://braindecode.org/auto_examples/plot_bcic_iv_2a_moabb_trial.html). As you will see, braindecode makes use of skorch, to avoid all the \"boilerplate\" code, usually necessary for training. In my opinion, writing those code is not as harm- and stressful as some people claim, so we will do it for our own purpose again. \n",
        "\n",
        "Libraries we are using (directly or indirectly) in this tutorial:\n",
        "- [Braindecode](https://braindecode.org/)\n",
        "- [PyTorch](https://pytorch.org/tutorials/)\n",
        "- [Pandas](https://pandas.pydata.org/docs/)\n",
        "- [Skorch](https://skorch.readthedocs.io/en/stable/)\n",
        "\n",
        "You do not have to know all those libraries. If you have trouble during the projects and need to work with them, you can still look into them. However, I assume that you can stick to a small set of libraries.\n"
      ],
      "metadata": {
        "id": "D8nG6WgBu3_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Braindecode Training"
      ],
      "metadata": {
        "id": "17Tbb-D5xsQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFz35s_QuqyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e6c512-b62a-466b-bc2f-54e2d7b8fdf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.4 MB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 177 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 155 kB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 30.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 242 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires coverage==3.7.1, but you have coverage 5.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "coveralls 0.5 requires coverage<3.999,>=3.6, but you have coverage 5.5 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install MNE, MOABB and Braindecode from github\n",
        "!pip --quiet install mne \n",
        "!pip --quiet install braindecode \n",
        "!pip --quiet install moabb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch\n",
        "import torch\n",
        "\n",
        "# Braindecode\n",
        "import braindecode\n",
        "from braindecode.datasets import MOABBDataset\n",
        "from braindecode.preprocessing import (\n",
        "    exponential_moving_standardize, preprocess, Preprocessor, scale)\n",
        "from braindecode.preprocessing import create_windows_from_events\n",
        "from braindecode import EEGClassifier\n",
        "from braindecode.util import set_random_seeds\n",
        "from braindecode.models import ShallowFBCSPNet\n",
        "\n",
        "# Skorch - fits well with scikit learn and the tensorflow like style of wrapping thins together\n",
        "from skorch.callbacks import LRScheduler\n",
        "from skorch.helper import predefined_split\n",
        "\n",
        "# Evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D \n",
        "import pandas as pd  # Creating a data frame from a history array\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from braindecode.visualization import plot_confusion_matrix"
      ],
      "metadata": {
        "id": "RXDNhWIzyFoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data handling\n",
        "\n",
        "At first, we load data as we did it in one of the previous tutorials via MOABB.\n",
        "For computational reasons, we will stick to two subjects (check the descriptions\n",
        "of the datasets if you want to know how many subjects were involved in the experiments). \n",
        "\n",
        "Following that, we apply some preprocessing to the data. At first, we are only interested in the EEG-data, so we **drop the stimuli and the MEG** electrodes. Furthermore, we **scale the the data**. Raw data is usually given in Volt. However, due to the \"weak\" electric fields, we are in a very small range. This is unhandy for deep learning, especially for regression tasks. Hence, we scale the data to micro Volt. Additionally **we filter very low and very high frequencies**, as both are unlikely to convey/ contain information to/ for the network. The last thing we apply is a **smoothing operation**, s.t. the network is confronted with smoothly varying functions instead with uncontinuously varying ones.\n",
        "\n",
        "Two last things remain to do with the data. The first thing is to **cut it into windows**. This increases the number of available samples and reduces computational complexity in terms of time and memory. Secondly, we will **split our data** into a validation and a test set."
      ],
      "metadata": {
        "id": "AxZJs3qs6A6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subject_ids = [1, 2]\n",
        "dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=subject_ids)\n",
        "\n",
        "# We prepocess the data\n",
        "\n",
        "# Frequency bands\n",
        "low_cut_hz = 4. \n",
        "high_cut_hz = 38. \n",
        "\n",
        "# Parameters for exponential moving standardization\n",
        "factor_new = 1e-3\n",
        "init_block_size = 1000\n",
        "\n",
        "preprocessors = [\n",
        "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  \n",
        "    Preprocessor(scale, factor=1e6, apply_on_array=True),  \n",
        "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  \n",
        "    Preprocessor(exponential_moving_standardize,  \n",
        "                 factor_new=factor_new, init_block_size=init_block_size)\n",
        "]\n",
        "\n",
        "preprocess(dataset, preprocessors)\n",
        "\n",
        "# Usually, you should check if the sampling frequency is the same for all datasets, \n",
        "# see the tutorial on the braindecode page for more infos\n",
        "trial_start_offset_seconds = -0.5\n",
        "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq) # convert from seconds to samples\n",
        "\n",
        "windows_dataset = create_windows_from_events(\n",
        "    dataset,\n",
        "    trial_start_offset_samples=trial_start_offset_samples,\n",
        "    trial_stop_offset_samples=0,\n",
        "    preload=True,\n",
        ")\n",
        "\n",
        "\n",
        "# The split operation from braindecode offers the option to split the datasets\n",
        "# according to the information in the datset info object.\n",
        "splitted = windows_dataset.split('session')\n",
        "train_set = splitted['session_T']\n",
        "valid_set = splitted['session_E']"
      ],
      "metadata": {
        "id": "9uBA2nYSx-WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is highly likely, that you do not want to load the data all the time, \n",
        "preprocess it and so on. Therefore, you can store the dataset to your drive \n",
        "(if you have enough memory there). \n",
        "\"\"\"\n",
        "train_set.save(\"/content/drive/MyDrive/AINS/data/training\", overwrite=True)\n",
        "valid_set.save(\"/content/drive/MyDrive/AINS/data/validation\", overwrite=True)\n",
        "\n",
        "\n",
        "train_set_loaded = braindecode.datautil.load_concat_dataset(\"/content/drive/MyDrive/AINS/data/training\", preload=True)\n",
        "print(train_set.description)\n",
        "print(train_set_loaded.description)"
      ],
      "metadata": {
        "id": "x9y8BriKZPGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and Hyperparameters\n",
        "Now, that we have prepared our dataset, we are ready to turn to the fun part, namely the deep learning setup. This is easy for our toy example. First, we check for the GPU. Afterwards, we set a **seed for reproducible results**. In order to set up the model, we need some information about the data set.\n",
        "\n",
        "1. Output dimension: Since we have a classification problem, we need the number of classes, 4 (left hand, right hand, feet, tongue) in our case. \n",
        "2. The number of input features: Equals the number of channels of our data\n",
        "3. How many input samples we have\n",
        "\n",
        "The model we are using is described in [1] (and is important for project 1 of the seminar). It is a **shallow convolutional network** - so nothing really deep here ;). Besides setting up the model, we will choose some **hyperparameters**. Hyperparameters are the settings one can adjust in order to improve the results. Examples are:\n",
        "- Network parameters: e.g. for convolutional networks the number of kernels/ filters, the kernel size, the stride ...\n",
        "- Optimization parameters: e.g. the concrete choice of the optimizer, the learning rate η, learning rate schedulers ...\n",
        "- Number of epochs (training time), batch size ...\n",
        "\n",
        "\n",
        "[1] Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F. & Ball, T. (2017). Deep learning with convolutional neural networks for EEG decoding and visualization. Human Brain Mapping , Aug. 2017"
      ],
      "metadata": {
        "id": "N-9HW65lB81S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "if cuda:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Seed\n",
        "seed = 20200220\n",
        "set_random_seeds(seed=seed, cuda=cuda)\n",
        "\n",
        "# Network params\n",
        "n_classes = 4\n",
        "n_chans = train_set[0][0].shape[0]\n",
        "input_window_samples = train_set[0][0].shape[1]\n",
        "\n",
        "model = ShallowFBCSPNet(\n",
        "    n_chans,\n",
        "    n_classes,\n",
        "    input_window_samples=input_window_samples,\n",
        "    final_conv_length='auto',\n",
        ")\n",
        "\n",
        "# Send model to GPU\n",
        "if cuda:\n",
        "    model.to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 0.000625 \n",
        "weight_decay = 0\n",
        "batch_size = 64\n",
        "n_epochs = 5"
      ],
      "metadata": {
        "id": "bS-22Tf-7hqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "We are nearly done. The only thing left is, that we wrap everything into the braindecode \"parent classes\": **EEGClassifier** and **EEGRegressor**. Both provide methods to train and predict in the **tensorflow way**, as I would call it. That is, you have methods like `fit()` and `predict()`. In combination with skorch, which enables e.g the usage of **callbacks**, the library becomes user-friendly and one can test ideas very fast. Overall, you just need to implement your own model. We will do this later as well. "
      ],
      "metadata": {
        "id": "KdAqjy4HLUSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We should pass the modules not instanciated (although it is possible to pass instanciated classes)\n",
        "state_dict = torch.load()   # Load state dict\n",
        "model = model.load_state_dict(state_dict)   # \n",
        "model.eval()\n",
        "\n",
        "clf = EEGClassifier(\n",
        "    model,   # our model\n",
        "    criterion=torch.nn.NLLLoss,  # We use a Negative Log LikeLihood Loss\n",
        "    optimizer=torch.optim.Adam, # Adam \n",
        "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
        "    optimizer__lr=lr,\n",
        "    optimizer__weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "    ],\n",
        "    device=device,\n",
        ")\n",
        "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
        "# in the dataset.\n",
        "clf.fit(train_set, y=None, epochs=n_epochs)"
      ],
      "metadata": {
        "id": "Y-sat8srK8zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "Now, that we have trained our model, we want to plot the results of the training process. The skorch framework provides a history object where the training progress is stored. As I simply adopted the braindecode tutorial, you are free to plot in a different style, with a different library and without the usage of pandas! It is already quite exhausting to work with torch, MNE and braindecode, so look what suits you the best and makes life easier for you!"
      ],
      "metadata": {
        "id": "ystMim0WzZKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The wrappers (EEGClassification/ EEGRegression) have a history attribute, \n",
        "# that contain the results. We can extraxt data from them\n",
        "results_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\n",
        "print(type(clf.history))\n",
        "df = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n",
        "                  index=clf.history[:, 'epoch'])\n",
        "\n",
        "# We can add new columns to the data frame: the misclassification is a good\n",
        "# quantity to countercheck the loss\n",
        "df = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n",
        "               valid_misclass=100 - 100 * df.valid_accuracy)\n",
        "\n",
        "# Styling is set as in the original Schirrmeister Paper\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "# Creating one single subplot\n",
        "fig, ax1 = plt.subplots(figsize=(8, 3))\n",
        "\n",
        "# We can access multiple columns in a data frame object and plot directly from \n",
        "# the data frame functionality .plot() into the created axes object ax1\n",
        "df.loc[:, ['train_loss', 'valid_loss']].plot(\n",
        "    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\n",
        "ax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n",
        "\n",
        "# We want to plot the missclassification values for comparison - therefore \n",
        "# we create a second axes which shares the same x-axes as ax1 (.twinx())\n",
        "ax2 = ax1.twinx()  \n",
        "\n",
        "# Afterwards, we use the same functionality for the plot of the misclassification\n",
        "# as for the loss values\n",
        "df.loc[:, ['train_misclass', 'valid_misclass']].plot(\n",
        "    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\n",
        "ax2.set_ylabel(\"Misclassification Rate [%]\", color='tab:red', fontsize=14)\n",
        "ax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\n",
        "ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
        "\n",
        "# We can modify the styling of the data plots (colors, linestyling etc)\n",
        "handles = []\n",
        "handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\n",
        "handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\n",
        "plt.legend(handles, [h.get_label() for h in handles], fontsize=14)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "UU8NstqNzYxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Confusion Matrix\n",
        "Something, that is used frequently in classification tasks are confusion matrices. They provide a good overview on how good the model actually performed besides the normal accuracy metric and the loss value. Wikipedia has a sufficient description in [this article](https://en.wikipedia.org/wiki/Confusion_matrix) if you need a recap or want to get familiar with the topic.\n",
        "\n",
        "We will create a confusion matrix from scikit learn and plot it with the inbuild visualization tool from braindecode. Hew, we predict with our EEGClassification wrapper, however, you could simply do this with your own routine and your own model. The results obtained can be used for the confusion matrix method from sklearn again, the methods here are useful even if you do not use the wrapper facilites of skorch. \n",
        "\n",
        "Within the confusion matrix (see the copy below), you can read off different metrics/ quantities. At first, we assume that the left side represents the actual observations, while the bottom side of the confusion matrix represents the predicted values. Within the squares we see two numbers: \n",
        "- The integer number, tells us how often a class was predicted to be the same class or, respectively, another class\n",
        "- The percentage number is the number of predictions with respect to the total number of samples in the set\n",
        "\n",
        "Furthermore, we see the recall (or true-positve rate)\n",
        "\n",
        "\\begin{equation} TPR = \\frac{TP}{TP + FN} \\end{equation} \n",
        "\n",
        "and the precision (or positve predictive value)\n",
        "\\begin{equation} PPV = \\frac{TP}{TP + FP} \\quad .\\end{equation} \n",
        "\n",
        "In the equations, the abbreviations stand for:\n",
        "- $TP$ : True positives - the number of samples that are classified to be present and are indeed present\n",
        "- $FN$ : False negatives - the number of samples that are classified to be not present but are\n",
        "- $FP$ : False positives - the number of samples that are classified to be present but aren't\n",
        "- $TN$ : True negatives (not contained above) - the number of samples that were correctly classified to be missing\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Mk0RTIYVMf5iKIm5dfNaEqOolz25oQ1o\" width=\"400\"/>\n",
        "\n",
        "<div text-align=center class=\"caption\">EEG 10-20 System (Source: Wikipedia) </div>\n",
        "</div>\n",
        "\n",
        "There are several other metrics that can be calculated from the above values. However, the values given here are sufficient and you can refer to the wikipedia article for more information. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8mKNutjNMDqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate confusion matrices\n",
        "# get the targets\n",
        "y_true = valid_set.get_metadata().target\n",
        "y_pred = clf.predict(valid_set)\n",
        "\n",
        "# generating confusion matrix\n",
        "confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# add class labels\n",
        "# label_dict is class_name : str -> i_class : int\n",
        "label_dict = valid_set.datasets[0].windows.event_id.items()\n",
        "# sort the labels by values (values are integer class labels)\n",
        "labels = list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys())\n",
        "\n",
        "# plot the basic conf. matrix\n",
        "plot_confusion_matrix(confusion_mat, class_names=labels)"
      ],
      "metadata": {
        "id": "P3CNm75pMXCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our own Deep Learning Pipeline\n",
        "\n",
        "As we are interested in applying deep learning techniques to EEG data, the way presented above is nice and allows for quick experiments. However, Braindecode is limited and one is likely interested in developing his own approaches to solve problems in this area. Therefore, we have to design our own experiments, models and training procedures. This leads us to the question, of how to create a pipeline that suits our needs the best. \n",
        "\n",
        "To address this, we will develop a small pipeline from PyTorch, by using ownly torch facilities. We will create our own model, design a small wrapper class around the training procedure and train the model. This is not new to everyone, but as we are from different backgrounds, this serves as a smooth introduction. I assume, that everyone has looked into the python and torch tutorial. Hence, I omit a detailed explanation of basic facts. If you have trouble understanding, just ask (or write a message on mattermost later :) ). \n",
        "\n"
      ],
      "metadata": {
        "id": "bCd_UCOWw9-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Deep Neural Network\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1hwoIpCdR8Xv9C4CAYOhzNPpwr7rdhbao\" height=\"650\"/>\n",
        "\n",
        "<div text-align=center class=\"caption\">Model Structure </div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "abKrhY_hntjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "As a first step, we will build a model. We use the \n",
        "\"\"\"\n",
        "class MyNetwork(torch.nn.Module):\n",
        "  def __init__(self, embedding_dim, steps, n_classes=4, name=\"MyModule\"):\n",
        "    super().__init__()\n",
        "\n",
        "    # Assume that the input is simply (batch_size, steps, n_channels)\n",
        "    # We choose some layers randomly ... (nothing special here, no model that was particularly successfull...) \n",
        "    \n",
        "    # projection to another space sounds great\n",
        "\n",
        "    # We use a residual block\n",
        "\n",
        "    # Batch Normalization\n",
        "\n",
        "    # Flatten the convolutional output \n",
        "\n",
        "    # Linear prediction layer\n",
        "\n",
        "    # Choose a nonlinearity\n",
        "\n",
        "    # We need an output probability distribution \n",
        "\n",
        "\n",
        "    print(f\"Model {name} successfully initialized\")\n",
        "\n",
        "\n",
        "    # We could also create a module list\n",
        "    # torch.nn.ModuleList()\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward method, the calculations of the model are executed\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    return y"
      ],
      "metadata": {
        "id": "WI4ZcQpAxiIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Datasets\n",
        "\n",
        "PyTorch is based on the usage of datasets. In this way, the user can design datasets for a very special purposes. This is very flexible. At the same time, the user is responsible for the efficient usage of computing power. However, the flexibility allows a lot of experiments and is therefore quite appropriate for our task. The dataset itself implements (overwrites) two methods:\n",
        "- `__getitem__()`: Allows indexing and gathering, the behaviour of iterables\n",
        "- `__len__()`: returns total number of available (input, output) tuples\n",
        "\n",
        "It is often convenient to wrap the dataset into a dataloader, which takes care of the iteration process and eases data handling. \n",
        "\n",
        "More information can be found [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) "
      ],
      "metadata": {
        "id": "NeNYS75ACFao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "class SampleDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, transform=None, target_transform=None):\n",
        "    self.input_data, self.labels = data  # For simplicity, we just pass a tuple, that is data=(input_data, labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_data.shape[0]  # return number of samples in the dataset\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_data[idx, :].unsqueeze(0), self.labels[idx]   # return data + label for given idx\n",
        "\n",
        "\n",
        "\n",
        "def get_data(n_samples, n_steps, t_max, min, max, type=\"sine\"):\n",
        "  if type == \"sine\":\n",
        "    fun = torch.sin\n",
        "  else:\n",
        "    fun = torch.cos\n",
        "  delta = min + max*torch.rand(size=(n_samples,), dtype=torch.float32)[..., None]\n",
        "  x = torch.linspace(0, t_max, n_steps, dtype=torch.float32)[None, ...]\n",
        "  x = x.repeat(n_samples, 1)\n",
        "  data_phase, labels_phase = fun(x + delta), torch.zeros(size=(x.shape[0],),  dtype=torch.long)\n",
        "  data_shift, labels_shift = fun(x) + delta, torch.ones(size=(x.shape[0],), dtype=torch.long)\n",
        "  data_freq, labels_freq = fun(delta*x), 2*torch.ones(size=(x.shape[0],), dtype=torch.long)\n",
        "\n",
        "  input_data = torch.concat([data_phase, data_shift, data_freq], dim=0)\n",
        "  labels = torch.concat([labels_phase, labels_shift, labels_freq], dim=0)\n",
        "  return input_data, labels\n",
        "\n",
        "# Create dataset\n",
        "n_samples = 10000\n",
        "n_steps = 200\n",
        "t_max = 10\n",
        "min = -5.\n",
        "max = 5.\n",
        "\n",
        "data, labels = get_data(n_samples, n_steps, t_max, min, max)\n",
        "\n",
        "\n",
        "# Inspect data\n",
        "plt.figure()\n",
        "plt.plot(data[10, :], label=\"phase\")\n",
        "plt.plot(data[7020, :], label=\"shift\")\n",
        "plt.plot(data[13100, :], label=\"freq\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "dataset = SampleDataset((data, labels))\n",
        "print(dataset[0][0].shape)\n",
        "\n",
        "\n",
        "# Check if data suits network\n",
        "model = MyNetwork(embedding_dim=100, steps=n_steps, n_classes=3, name=\"SineClassifier\")\n",
        "x, y = data[0:4, :], labels[0:4]\n",
        "x = x.unsqueeze(dim=1)\n",
        "print(x.shape)\n",
        "out = model(x)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "ksBjQXKiCBQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything together\n",
        "\n",
        "We will do, what braindecode did before. We **implement a small wrapper class** to enable fast training within scripts. In this way, we can create a class that provides all the methods we need for our task, but can reuse it easily in several scripts or if we are interested in hyperparameter tuning. This is not necessary, you can also write methods and just call them. For me, I found it very useful to have a class that enables this tensorflow logic. Sure, we could also stick to braindecode. In this way, however, we can easily modify the wrapper. "
      ],
      "metadata": {
        "id": "0paV5PqDpdgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainWrapper:\n",
        "  seed = 725\n",
        "  def __init__(self, dataset, model, optimizer, loss, batch_size, scheduler=None):\n",
        "    # Split data into train, val set\n",
        "    train_length = int(0.85 * len(dataset))\n",
        "    val_length = len(dataset) - train_length\n",
        "    train, val = torch.utils.data.random_split(dataset=dataset, lengths=[train_length, val_length], generator=torch.Generator().manual_seed(self.seed))\n",
        "\n",
        "    # For iterable datasets, torch provides data loaders. With them, you can tune the GPU usage by increasing the batch size\n",
        "    # and increasing the number of workers. Below you will see, that it behaves somewhat like an python iterator\n",
        "    self.train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "    self.val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    # Save optimizer, loss function and scheduler (and a metric if you want)\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss\n",
        "    self.scheduler = scheduler\n",
        "    \n",
        "    # If the GPU is available, we move the model to cuda and store it to a variable\n",
        "    self.model = model.to(\"cuda\") if torch.cuda.is_available() else model\n",
        "    self.print_model()\n",
        "\n",
        "    # Lists where the loss values are saved\n",
        "    self.train_loss = []\n",
        "    self.val_loss = []\n",
        "\n",
        "  def val_set():\n",
        "    return self.val_loader.dataset.input_data, self.val_loader.dataset.input_data\n",
        "  \n",
        "  def count_parameters(self):\n",
        "    return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "  def print_model(self):\n",
        "    print(f\"Number of model parameters: {self.count_parameters()}\")\n",
        "    print(self.model)\n",
        "\n",
        "  def train_step(self):\n",
        "    \"\"\"\n",
        "    The train step: here we can \n",
        "    \"\"\"\n",
        "    loss_val = 0\n",
        "    self.model.train(True)\n",
        "    for i, (x, y) in enumerate(self.train_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        x, y = x.to(\"cuda\"), y.to(\"cuda\") \n",
        "      y_pred = model(x)\n",
        "      batch_loss = self.loss_fn(y_pred, y)\n",
        "\n",
        "      loss_val += batch_loss.item()\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    self.train_loss.append(loss_val / (i+1))\n",
        "\n",
        "  def val_step(self):\n",
        "    loss_val = 0\n",
        "    self.model.train(False)\n",
        "    for i, (x, y) in enumerate(self.val_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        x, y = x.to(\"cuda\"), y.to(\"cuda\") \n",
        "      y_pred = model(x)\n",
        "      batch_loss = self.loss_fn(y_pred, y)\n",
        "\n",
        "      loss_val += batch_loss.item()\n",
        "\n",
        "    self.val_loss.append(loss_val / (i+1))\n",
        "\n",
        "  def fit(self, n_epochs=10):\n",
        "    print(\"Epoch\\tTrain Loss\\tVal. Loss\\tLearning Rate\\n\"\n",
        "          \"-----\\t----------\\t---------\\t-------------\")\n",
        "    for epoch in range(n_epochs):\n",
        "      self.train_step()\n",
        "      self.val_step()\n",
        "      self.scheduler.step()\n",
        "\n",
        "      print(f\"{epoch:>5d}\\t{self.train_loss[-1]:>2.7f}\\t{self.val_loss[-1]:>.7f}\\t{self.scheduler.get_last_lr()}\")\n"
      ],
      "metadata": {
        "id": "813n2eymp0-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=4, gamma=0.25)\n",
        "wrapper = TrainWrapper(dataset, model, optimizer=optim, loss=loss_fn, batch_size=64, scheduler=scheduler)\n",
        "wrapper.fit(n_epochs=20)\n",
        "\n",
        "model = wrapper.model\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(wrapper.train_loss, label=\"Training Loss\")\n",
        "plt.plot(wrapper.val_loss, label=\"Validation Loss\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Loss Plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GdLUYnMZCEM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Network"
      ],
      "metadata": {
        "id": "ZlH-JSy8pFK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can save our model to the drive (you should do this for evaluation reasons)\n",
        "# Below we save the state dict - this are the layer names plus their state (weights, bias ...)\n",
        "# torch.save(model.state_dict(), f=\"/content/drive/MyDrive/AINS/models/MyModel/my_model.pth\")\n",
        "\n",
        "# Now we can load the state dict\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/AINS/models/MyModel/my_model.pth\")\n",
        "\n",
        "# The state dict are just the parameters - we need to recreate the model with random \n",
        "# weights\n",
        "loaded_model = MyNetwork(embedding_dim=100, steps=n_steps, n_classes=3, name=\"SineClassifier\")\n",
        "loaded_model.load_state_dict(state_dict)\n",
        "loaded_model.eval()   # Always call evaluate if you use e.g. dropout, otherwise you will get \"wrong\" results\n",
        "\n",
        "# Create test data\n",
        "n_samples = 50\n",
        "n_steps = 200\n",
        "t_max = 10\n",
        "min = -8.\n",
        "max = 8.\n",
        "\n",
        "test_data = get_data(n_samples, n_steps, t_max, min, max, type=\"cos\")\n",
        "test_input = test_data[0]\n",
        "test_labels = test_data[1]\n",
        "\n",
        "# We pass the whole training set in one run\n",
        "pred = loaded_model(test_input.unsqueeze(1))\n",
        "\n",
        "# Generating the confusion matrix again\n",
        "confusion_mat = confusion_matrix(test_labels, torch.argmax(pred.detach(), dim=-1))\n",
        "\n",
        "# add class labels\n",
        "label_dict = [\"Phase\", \"Shift\", \"Scale\"]\n",
        "\n",
        "# plot the basic conf. matrix\n",
        "print(\"Confusion Matrix\\n\"\n",
        "      \"----------------\")\n",
        "plot_confusion_matrix(confusion_mat, class_names=label_dict)\n",
        "\n",
        "\n",
        "# Inspect intermediate results\n",
        "embedding = loaded_model.embedding_layer(test_input[0:126:25].unsqueeze(1))\n",
        "convolved = loaded_model.convolution_0(embedding)\n",
        "convolved = loaded_model.convolution_1(convolved)\n",
        "convolved = loaded_model.convolution_2(convolved)\n",
        "convolved = loaded_model.convolution_3(convolved)\n",
        "print(convolved.shape)\n",
        "\n",
        "convolved = convolved.detach()\n",
        "\"\"\"\n",
        "fig, axs = plt.subplots(ncols=4, nrows=8, figsize=(15, 30))\n",
        "for i in range(8):\n",
        "  for j in range(4):\n",
        "    axs[i, j].plot(convolved[0, i*4 + j, :], label=\"Channel \"+ str(i*4 + j))\n",
        "    axs[i, j].grid()\n",
        "plt.grid()\n",
        "plt.show   \n",
        "\"\"\"\n",
        "print(\"\\nLayer Outputs\\n\"\n",
        "      \"----------------\")\n",
        "fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(15, 3))\n",
        "axs[0].plot(convolved[0, 0], 'b-.', label=\"Phase\")\n",
        "axs[0].plot(convolved[1, 0], 'b--', label=\"Phase\")\n",
        "axs[0].grid()\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(convolved[2, 0], 'k-.', label=\"Shift\")\n",
        "axs[1].plot(convolved[3, 0], 'k--', label=\"Shift\")\n",
        "axs[1].grid()\n",
        "axs[1].legend()\n",
        "\n",
        "axs[2].plot(convolved[4, 0], 'r-.', label=\"Scale\")\n",
        "axs[2].plot(convolved[5, 0], 'r--', label=\"Scale\")\n",
        "axs[2].grid()\n",
        "axs[2].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCOHBW8uOAuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus - Latent Spaces and UMAP\n",
        "\n",
        "We have a very comprehensive look into the UMAP. I just want to show, that this is working as it is important if you are working on the SSL project, where you want to create meaningful latent spaces. The visualizations in the Banville paper from project 2 are very interesting and also created with UMAP. \n",
        "\n",
        "UMAP is a dimension reduction algorithm, that embedds a high-dimensional data structure into two dimensions - everything based on topology and fuzzy theory. If you are interested, you can start with the description on the [UMAP page](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html). We will not deal with that here (I am still trying to figure out how this works).\n"
      ],
      "metadata": {
        "id": "jFK0LgmZ1CoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/lmcinnes/umap.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPTnunVAw865",
        "outputId": "31c43927-7739-49d1-b565-7031f790fab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lmcinnes/umap.git\n",
            "  Cloning https://github.com/lmcinnes/umap.git to /tmp/pip-req-build-6e71s45e\n",
            "  Running command git clone -q https://github.com/lmcinnes/umap.git /tmp/pip-req-build-6e71s45e\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.2) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.2) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.2) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.2) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.5.2) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn==0.5.2) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn==0.5.2) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn==0.5.2) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn==0.5.2) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82830 sha256=94ed80a72240b9353141afe7a545e33669cb6dbb47dec0aa4ce985ef84f10b6c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-czznzmz7/wheels/8e/8f/23/7b32f2bbe743ffefedd425e60aa259e296c823ab6cc5d14e5b\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=1bbfce143a130753ce6ebfd003da56d16f5459711caaaaf06277b96624dbc789\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "VzyJs3brxBDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reducer = umap.UMAP(random_state=42)\n",
        "reducer.fit(test_input)\n",
        "embedding = reducer.transform(test_input)"
      ],
      "metadata": {
        "id": "6n8zmK2LxD3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9596440b-62ba-4cfc-e090-7677e497561c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=test_labels, cmap='viridis', s=5)\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.colorbar(boundaries=np.arange(4)).set_ticks(np.arange(1, 4))\n",
        "plt.title('UMAP projection of the sine waves dataset', fontsize=16);"
      ],
      "metadata": {
        "id": "aT8V4jhOzjPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_embedding = loaded_model.embedding_layer(test_input)\n",
        "nn_embedding = nn_embedding.detach()\n",
        "\n",
        "reducer = umap.UMAP(random_state=42)\n",
        "reducer.fit(nn_embedding)\n",
        "umap_embedding = reducer.transform(nn_embedding)\n",
        "\n",
        "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=test_labels, cmap='viridis', s=5)\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.colorbar(boundaries=np.arange(4)).set_ticks(np.arange(1, 4))\n",
        "plt.title('UMAP projection of the sine waves dataset', fontsize=16);"
      ],
      "metadata": {
        "id": "zyt0jaFZz3_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = loaded_model.embedding_layer(test_input.unsqueeze(1))\n",
        "convolved = loaded_model.convolution_0(embedding)\n",
        "convolved = loaded_model.convolution_1(convolved)\n",
        "convolved = loaded_model.convolution_2(convolved)\n",
        "convolved = loaded_model.convolution_3(convolved)\n",
        "convolved = convolved.detach().squeeze(1)\n",
        "\n",
        "reducer = umap.UMAP(random_state=42)\n",
        "reducer.fit(convolved)\n",
        "umap_embedding = reducer.transform(convolved)\n",
        "\n",
        "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=test_labels, cmap='viridis', s=5)\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.colorbar(boundaries=np.arange(4)).set_ticks(np.arange(1, 4))\n",
        "plt.title('UMAP projection of the sine waves dataset', fontsize=16);"
      ],
      "metadata": {
        "id": "B6idNYZK1dBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping to non-euclidean distances\n",
        "sphere_mapper = umap.UMAP(output_metric='haversine', random_state=42).fit(convolved)\n",
        "x = np.sin(sphere_mapper.embedding_[:, 0]) * np.cos(sphere_mapper.embedding_[:, 1])\n",
        "y = np.sin(sphere_mapper.embedding_[:, 0]) * np.sin(sphere_mapper.embedding_[:, 1])\n",
        "z = np.cos(sphere_mapper.embedding_[:, 0])\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=test_labels, cmap='Spectral')\n"
      ],
      "metadata": {
        "id": "L1YxBEvpsVDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}